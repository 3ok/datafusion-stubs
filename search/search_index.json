{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Apache DataFusion Python Experimental Docs","text":""},{"location":"#table-of-contents","title":"Table of Contents","text":"<ul> <li>Installation</li> <li>Getting Started</li> <li>API Reference</li> </ul>"},{"location":"#installation","title":"Installation","text":"<p>To install DataFusion Python bindings, you can use pip:</p> <pre><code>pip install datafusion\n</code></pre> <p>Note</p> <p>If you want to have better developer experience, you can install the experimental type stubs package.</p> <pre><code>pip install datafusion-stubs\n</code></pre>"},{"location":"#getting-started","title":"Getting Started","text":""},{"location":"#creating-a-sessioncontext","title":"Creating a <code>SessionContext</code>","text":"<p>Start by importing DataFusion and creating a <code>SessionContext</code> <pre><code>import datafusion\nimport pyarrow\n\nfrom datafusion import col\n\nctx = datafusion.SessionContext()\n</code></pre></p>"},{"location":"#loading-data","title":"Loading data","text":"<p>For demonstration purposes, let's create a simple DataFrame manually. In practice, you might load data from a file.</p> <pre><code>batch = pyarrow.RecordBatch.from_arrays(\n    [\n        pyarrow.array([1, 2, 3]),\n        pyarrow.array([4, 5, 6]),\n    ],\n    names=[\"a\", \"b\"],\n)\n\ndf = ctx.create_dataframe([[batch]], name=\"batch_array\")\n</code></pre>"},{"location":"#creating-a-new-statement","title":"Creating a new statement","text":"<p>You can use DataFusion Expressions to build up a query definition <pre><code>df = df.select(\n    col(\"a\") + col(\"b\"),\n    col(\"a\") - col(\"b\"),\n)\n</code></pre></p>"},{"location":"#viewing-the-results","title":"Viewing the results","text":"<pre><code>df\n</code></pre> <pre><code>DataFrame()\n+-------------------------------+-------------------------------+\n| batch_array.a + batch_array.b | batch_array.a - batch_array.b |\n+-------------------------------+-------------------------------+\n| 5                             | -3                            |\n| 7                             | -3                            |\n| 9                             | -3                            |\n+-------------------------------+-------------------------------+\n</code></pre>"},{"location":"api/context/","title":"Context","text":""},{"location":"api/context/#datafusion.SessionContext","title":"<code>datafusion.SessionContext</code>","text":""},{"location":"api/context/#datafusion.SessionContext-functions","title":"Functions","text":""},{"location":"api/context/#datafusion.SessionContext.__init__","title":"<code>datafusion.SessionContext.__init__(config=None, runtime=None)</code>","text":"<p>Main interface for executing queries with DataFusion.</p> <p>Maintains the state of the connection between a user and an instance of the connection between a user and an instance of the DataFusion engine.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>SessionConfig | None</code> <p>Session configuration options.</p> <code>None</code> <code>runtime</code> <code>RuntimeConfig | None</code> <p>Runtime configuration options.</p> <code>None</code> <p>Examples:</p> <p>The following example demostrates how to use the context to execute a query against a CSV data source using the <code>DataFrame</code> API:</p> <pre><code>from datafusion import SessionContext\n\nctx = SessionContext()\ndf = ctx.read_csv(\"data.csv\")\n</code></pre>"},{"location":"api/context/#datafusion.SessionContext.catalog","title":"<code>datafusion.SessionContext.catalog(name='datafusion')</code>","text":"<p>Retrieve a catalog by name.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the catalog to retrieve, by default \"datafusion\".</p> <code>'datafusion'</code> <p>Returns:</p> Type Description <code>Catalog</code> <p>Catalog representation.</p>"},{"location":"api/context/#datafusion.SessionContext.create_dataframe_from_logical_plan","title":"<code>datafusion.SessionContext.create_dataframe_from_logical_plan(plan)</code>","text":"<p>Create a <code>DataFrame</code> from an existing logical plan.</p> <p>Parameters:</p> Name Type Description Default <code>plan</code> <code>LogicalPlan</code> <p>Logical plan.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame representation of the logical plan.</p>"},{"location":"api/context/#datafusion.SessionContext.empty_table","title":"<code>datafusion.SessionContext.empty_table()</code>","text":"<p>Create an empty <code>DataFrame</code>.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>An empty DataFrame.</p>"},{"location":"api/context/#datafusion.SessionContext.from_arrow_table","title":"<code>datafusion.SessionContext.from_arrow_table(data, name)</code>","text":"<p>Create a <code>DataFrame</code> from an Arrow table.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Table</code> <p>Arrow table.</p> required <code>name</code> <code>str | None</code> <p>Name of the DataFrame.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame representation of the Arrow table.</p>"},{"location":"api/context/#datafusion.SessionContext.from_pandas","title":"<code>datafusion.SessionContext.from_pandas(data, name)</code>","text":"<p>Create a <code>DataFrame</code> from a Pandas DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>Pandas DataFrame.</p> required <code>name</code> <code>str | None</code> <p>Name of the DataFrame.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame representation of the Pandas DataFrame.</p>"},{"location":"api/context/#datafusion.SessionContext.from_polars","title":"<code>datafusion.SessionContext.from_polars(data, name)</code>","text":"<p>Create a <code>DataFrame</code> from a Polars DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>Polars DataFrame.</p> required <code>name</code> <code>str | None</code> <p>Name of the DataFrame.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame representation of the Polars DataFrame.</p>"},{"location":"api/context/#datafusion.SessionContext.from_pydict","title":"<code>datafusion.SessionContext.from_pydict(data, name)</code>","text":"<p>Create a <code>DataFrame</code> from a dictionary of lists.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>dict[str, list[Any]]</code> <p>Dictionary of lists.</p> required <code>name</code> <code>str | None</code> <p>Name of the DataFrame.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame representation of the dictionary of lists.</p>"},{"location":"api/context/#datafusion.SessionContext.from_pylist","title":"<code>datafusion.SessionContext.from_pylist(data, name)</code>","text":"<p>Create a <code>DataFrame</code> from a list of dictionaries.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>list[dict[str, Any]]</code> <p>List of dictionaries.</p> required <code>name</code> <code>str | None</code> <p>Name of the DataFrame.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame representation of the list of dictionaries.</p>"},{"location":"api/context/#datafusion.SessionContext.read_avro","title":"<code>datafusion.SessionContext.read_avro(path, schema=None, file_partition_cols=..., file_extension='.avro')</code>","text":"<p>Create a <code>DataFrame</code> for reading Avro data source.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the Avro file</p> required <code>schema</code> <code>Schema | None</code> <p>The data source schema, by default None</p> <code>None</code> <code>file_partition_cols</code> <code>list[tuple[str, str]]</code> <p>Partition columns, by default ...</p> <code>...</code> <code>file_extension</code> <code>str</code> <p>File extension to select, by default \".avro\"</p> <code>'.avro'</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame representation of the read Avro file</p>"},{"location":"api/context/#datafusion.SessionContext.read_csv","title":"<code>datafusion.SessionContext.read_csv(path, schema=None, has_header=True, delimiter=',', schema_infer_max_records=1000, file_extension='.csv', table_partition_cols=..., file_compression_type=None)</code>","text":"<p>Create a <code>DataFrame</code> for reading a CSV data source.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the CSV file</p> required <code>schema</code> <code>Schema | None</code> <p>An optional schema representing the CSV files. If None, the CSV reader will try to infer it based on data in file, by default None</p> <code>None</code> <code>has_header</code> <code>bool</code> <p>Whether the CSV file have a header. If schema inference is run on a file with no headers, default column names are created, by default True</p> <code>True</code> <code>delimiter</code> <code>str</code> <p>An optional column delimiter, by default \",\"</p> <code>','</code> <code>schema_infer_max_records</code> <code>int</code> <p>Maximum number of rows to read from CSV files for schema inference if needed, by default 1000</p> <code>1000</code> <code>file_extension</code> <code>str</code> <p>File extension; only files with this extension are selected for data input, by default \".csv\"</p> <code>'.csv'</code> <code>table_partition_cols</code> <code>list[tuple[str, str]]</code> <p>Partition columns, by default ...</p> <code>...</code> <code>file_compression_type</code> <code>str | None</code> <p>File compression type, by default None</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame representation of the read CSV files</p>"},{"location":"api/context/#datafusion.SessionContext.read_json","title":"<code>datafusion.SessionContext.read_json(path, schema=None, schema_infer_max_records=1000, file_extension='.json', table_partition_cols=..., file_compression_type=None)</code>","text":"<p>Create a <code>DataFrame</code> for reading a line-delimited JSON data source.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the JSON file</p> required <code>schema</code> <code>Schema | None</code> <p>The data source schema, by default None</p> <code>None</code> <code>schema_infer_max_records</code> <code>int</code> <p>Maximum number of rows to read from JSON files for schema inference if needed, by default 1000</p> <code>1000</code> <code>file_extension</code> <code>str</code> <p>File extension; only files with this extension are selected for data input, by default \".json\"</p> <code>'.json'</code> <code>table_partition_cols</code> <code>list[tuple[str, str]]</code> <p>Partition columns, by default ...</p> <code>...</code> <code>file_compression_type</code> <code>str | None</code> <p>File compression type, by default None</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame representation of the read JSON files</p>"},{"location":"api/context/#datafusion.SessionContext.read_parquet","title":"<code>datafusion.SessionContext.read_parquet(path, table_partition_cols=..., parquet_pruning=True, file_extension='.parquet', skip_metadata=True, schema=None, file_sort_order=None)</code>","text":"<p>Create a <code>DataFrame</code> for reading Parquet data source.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the Parquet file</p> required <code>table_partition_cols</code> <code>list[tuple[str, str]]</code> <p>Partition columns, by default ...</p> <code>...</code> <code>parquet_pruning</code> <code>bool</code> <p>Whether the parquet reader should use the predicate to prune row groups, by default True</p> <code>True</code> <code>file_extension</code> <code>str</code> <p>File extension; only files with this extension are selected for data input, by default \".parquet\"</p> <code>'.parquet'</code> <code>skip_metadata</code> <code>bool</code> <p>Whether the parquet reader should skip any metadata that may be in the file schema. This can help avoid schema conflicts due to metadata. by default True</p> <code>True</code> <code>schema</code> <code>Schema | None</code> <p>An optional schema representing the parquet files. If None, the parquet reader will try to infer it based on data in the file, by default None</p> <code>None</code> <code>file_sort_order</code> <code>list[list[Expr]] | None</code> <p>Sort order for the file, by default None</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame representation of the read Parquet files</p>"},{"location":"api/context/#datafusion.SessionContext.register_avro","title":"<code>datafusion.SessionContext.register_avro(name, path, schema=None, file_extension='.avro', table_partition_cols=...)</code>","text":"<p>Register an Avro file as a table.</p> <p>The registered table can be referenced from SQL statement executed against this context.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the table to register.</p> required <code>path</code> <code>str</code> <p>Path to the Avro file.</p> required <code>schema</code> <code>Schema | None</code> <p>The data source schema, by default None</p> <code>None</code> <code>file_extension</code> <code>str</code> <p>File extension to select, by default \".avro\"</p> <code>'.avro'</code> <code>table_partition_cols</code> <code>list[tuple[str, str]]</code> <p>Partition columns, by default ...</p> <code>...</code>"},{"location":"api/context/#datafusion.SessionContext.register_csv","title":"<code>datafusion.SessionContext.register_csv(name, path, schema=None, has_header=True, delimiter=',', schema_infer_max_records=1000, file_extension='.csv', file_compression_type=None)</code>","text":"<p>Register a CSV file as a table.</p> <p>The registered table can be referenced from SQL statement executed against.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the table to register.</p> required <code>path</code> <code>str</code> <p>Path to the CSV file.</p> required <code>schema</code> <code>Schema | None</code> <p>An optional schema representing the CSV file. If None, the CSV reader will try to infer it based on data in file, by default None</p> <code>None</code> <code>has_header</code> <code>bool</code> <p>Whether the CSV file have a header. If schema inference is run on a file with no headers, default column names are created, by default True</p> <code>True</code> <code>delimiter</code> <code>str</code> <p>An optional column delimiter, by default \",\"</p> <code>','</code> <code>schema_infer_max_records</code> <code>int</code> <p>Maximum number of rows to read from CSV files for schema inference if needed, by default 1000</p> <code>1000</code> <code>file_extension</code> <code>str</code> <p>File extension; only files with this extension are selected for data input, by default \".csv\"</p> <code>'.csv'</code> <code>file_compression_type</code> <code>str | None</code> <p>File compression type, by default None</p> <code>None</code>"},{"location":"api/context/#datafusion.SessionContext.register_dataset","title":"<code>datafusion.SessionContext.register_dataset(name, dataset)</code>","text":"<p>Register a <code>pyarrow.dataset.Dataset</code> as a table.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the table to register.</p> required <code>dataset</code> <code>Dataset</code> <p>PyArrow dataset.</p> required"},{"location":"api/context/#datafusion.SessionContext.register_json","title":"<code>datafusion.SessionContext.register_json(name, path, schema=None, schema_infer_max_records=1000, file_extension='.json', table_partition_cols=..., file_compression_type=None)</code>","text":"<p>Register a JSON file as a table.</p> <p>The registered table can be referenced from SQL statement executed against this context.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the table to register.</p> required <code>path</code> <code>str</code> <p>Path to the JSON file.</p> required <code>schema</code> <code>Schema | None</code> <p>The data source schema, by default None</p> <code>None</code> <code>schema_infer_max_records</code> <code>int</code> <p>Maximum number of rows to read from JSON files for schema inference if needed, by default 1000</p> <code>1000</code> <code>file_extension</code> <code>str</code> <p>File extension; only files with this extension are selected for data input, by default \".json\"</p> <code>'.json'</code> <code>table_partition_cols</code> <code>list[tuple[str, str]]</code> <p>Partition columns, by default ...</p> <code>...</code> <code>file_compression_type</code> <code>str | None</code> <p>File compression type, by default None</p> <code>None</code>"},{"location":"api/context/#datafusion.SessionContext.register_parquet","title":"<code>datafusion.SessionContext.register_parquet(name, path, table_partition_cols=..., parquet_pruning=True, file_extension='.parquet', skip_metadata=True, schema=None, file_sort_order=None)</code>","text":"<p>Register a Parquet file as a table.</p> <p>The registered table can be referenced from SQL statement executed against this context.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the table to register.</p> required <code>path</code> <code>str</code> <p>Path to the Parquet file.</p> required <code>table_partition_cols</code> <code>list[tuple[str, str]]</code> <p>Partition columns, by default ...</p> <code>...</code> <code>parquet_pruning</code> <code>bool</code> <p>Whether the parquet reader should use the predicate to prune row groups, by default True</p> <code>True</code> <code>file_extension</code> <code>str</code> <p>File extension; only files with this extension are selected for data input, by default \".parquet\"</p> <code>'.parquet'</code> <code>skip_metadata</code> <code>bool</code> <p>Whether the parquet reader should skip any metadata that may be in the file schema. This can help avoid schema conflicts due to metadata. by default True</p> <code>True</code> <code>schema</code> <code>Schema | None</code> <p>The data source schema, by default None</p> <code>None</code> <code>file_sort_order</code> <code>list[list[Expr]] | None</code> <p>Sort order for the file, by default None</p> <code>None</code>"},{"location":"api/context/#datafusion.SessionContext.register_udaf","title":"<code>datafusion.SessionContext.register_udaf(udaf)</code>","text":"<p>Register a user-defined aggregation function (UDAF) with the context.</p> <p>Parameters:</p> Name Type Description Default <code>udaf</code> <code>AggregateUDF</code> <p>User-defined aggregation function.</p> required"},{"location":"api/context/#datafusion.SessionContext.register_udf","title":"<code>datafusion.SessionContext.register_udf(udf)</code>","text":"<p>Register a user-defined function (UDF) with the context.</p> <p>Parameters:</p> Name Type Description Default <code>udf</code> <code>ScalarUDF</code> <p>User-defined function.</p> required"},{"location":"api/context/#datafusion.SessionContext.session_id","title":"<code>datafusion.SessionContext.session_id()</code>","text":"<p>Retrun an id that uniquely identifies this <code>SessionContext</code>.</p> <p>Returns:</p> Type Description <code>str</code> <p>Unique session identifier</p>"},{"location":"api/context/#datafusion.SessionContext.sql","title":"<code>datafusion.SessionContext.sql(query)</code>","text":"<p>Create a <code>DataFrame</code> from SQL query text.</p> <p>Note: This API implements DDL statements such as <code>CREATE TABLE</code> and <code>CREATE VIEW</code> and DML statements such as <code>INSERT INTO</code> with in-memory default implementation. See <code>SessionContext.sql_with_options</code>.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>SQL query text.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame representation of the SQL query.</p>"},{"location":"api/context/#datafusion.SessionContext.sql_with_options","title":"<code>datafusion.SessionContext.sql_with_options(query, options)</code>","text":"<p>Create a <code>DataFrame</code> from SQL query text, first validating that the query is allowed by the provided options.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>SQL query text.</p> required <code>options</code> <code>SQLOptions</code> <p>SQL options.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame representation of the SQL query.</p>"},{"location":"api/context/#datafusion.SessionContext.table","title":"<code>datafusion.SessionContext.table(name)</code>","text":"<p>Retrieve a <code>DataFrame</code> representing a previously registered table.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the table to retrieve.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame representation of the table.</p>"},{"location":"api/context/#datafusion.SessionContext.table_exist","title":"<code>datafusion.SessionContext.table_exist(name)</code>","text":"<p>Return whether a table with the given name exists.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the table to check.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>Whether a table with the given name exists.</p>"},{"location":"api/context/#datafusion.SessionConfig","title":"<code>datafusion.SessionConfig</code>","text":""},{"location":"api/context/#datafusion.SessionConfig-functions","title":"Functions","text":""},{"location":"api/context/#datafusion.SessionConfig.__init__","title":"<code>datafusion.SessionConfig.__init__(config_options)</code>","text":"<p>Create a new <code>SessionConfig</code> with the given configuration options.</p> <p>Parameters:</p> Name Type Description Default <code>config_options</code> <code>dict[str, str]</code> <p>Configuration options.</p> required"},{"location":"api/context/#datafusion.SessionConfig.set","title":"<code>datafusion.SessionConfig.set(key, value)</code>","text":"<p>Set a configuration option.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>Option key.</p> required <code>value</code> <code>str</code> <p>Option value.</p> required <p>Returns:</p> Type Description <code>SessionConfig</code> <p>A new <code>SessionConfig</code> object with the updated setting.</p>"},{"location":"api/context/#datafusion.SessionConfig.with_batch_size","title":"<code>datafusion.SessionConfig.with_batch_size(batch_size)</code>","text":"<p>Customize batch size.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>Batch size.</p> required <p>Returns:</p> Type Description <code>SessionConfig</code> <p>A new <code>SessionConfig</code> object with the updated setting.</p>"},{"location":"api/context/#datafusion.SessionConfig.with_create_default_catalog_and_schema","title":"<code>datafusion.SessionConfig.with_create_default_catalog_and_schema(enabled)</code>","text":"<p>Control whether the default catalog and schema will be automatically created.</p> <p>Parameters:</p> Name Type Description Default <code>enabled</code> <code>bool</code> <p>Whether the default catalog and schema will be automatically created.</p> required <p>Returns:</p> Type Description <code>SessionConfig</code> <p>A new <code>SessionConfig</code> object with the updated setting.</p>"},{"location":"api/context/#datafusion.SessionConfig.with_default_catalog_and_schema","title":"<code>datafusion.SessionConfig.with_default_catalog_and_schema(catalog, schema)</code>","text":"<p>Select a name for the default catalog and shcema.</p> <p>Parameters:</p> Name Type Description Default <code>catalog</code> <code>str</code> <p>Catalog name.</p> required <code>schema</code> <code>str</code> <p>Schema name.</p> required <p>Returns:</p> Type Description <code>SessionConfig</code> <p>A new <code>SessionConfig</code> object with the updated setting.</p>"},{"location":"api/context/#datafusion.SessionConfig.with_information_schema","title":"<code>datafusion.SessionConfig.with_information_schema(enabled)</code>","text":"<p>Enable or disable the inclusion of <code>information_schema</code> virtual tables.</p> <p>Parameters:</p> Name Type Description Default <code>enabled</code> <code>bool</code> <p>Whether to include <code>information_schema</code> virtual tables.</p> required <p>Returns:</p> Type Description <code>SessionConfig</code> <p>A new <code>SessionConfig</code> object with the updated setting.</p>"},{"location":"api/context/#datafusion.SessionConfig.with_parquet_pruning","title":"<code>datafusion.SessionConfig.with_parquet_pruning(enabled)</code>","text":"<p>Enable or disable the use of pruning predicate for parquet readers to skip row groups.</p> <p>Parameters:</p> Name Type Description Default <code>enabled</code> <code>bool</code> <p>Whether to use pruning predicate for parquet readers.</p> required <p>Returns:</p> Type Description <code>SessionConfig</code> <p>A new <code>SessionConfig</code> object with the updated setting.</p>"},{"location":"api/context/#datafusion.SessionConfig.with_repartition_aggregations","title":"<code>datafusion.SessionConfig.with_repartition_aggregations(enabled)</code>","text":"<p>Enable or disable the use of repartitioning for aggregations.</p> <p>Enabling this improves parallelism.</p> <p>Parameters:</p> Name Type Description Default <code>enabled</code> <code>bool</code> <p>Whether to use repartitioning for aggregations.</p> required <p>Returns:</p> Type Description <code>SessionConfig</code> <p>A new <code>SessionConfig</code> object with the updated setting.</p>"},{"location":"api/context/#datafusion.SessionConfig.with_repartition_file_min_size","title":"<code>datafusion.SessionConfig.with_repartition_file_min_size(size)</code>","text":"<p>Set minimum file range size for repartitioning scans.</p> <p>Parameters:</p> Name Type Description Default <code>size</code> <code>int</code> <p>Minimum file range size.</p> required <p>Returns:</p> Type Description <code>SessionConfig</code> <p>A new <code>SessionConfig</code> object with the updated setting.</p>"},{"location":"api/context/#datafusion.SessionConfig.with_repartition_file_scans","title":"<code>datafusion.SessionConfig.with_repartition_file_scans(enabled)</code>","text":"<p>Enable or disable the use of repartitioning for file scans.</p> <p>Parameters:</p> Name Type Description Default <code>enabled</code> <code>bool</code> <p>Whether to use repartitioning for file scans.</p> required <p>Returns:</p> Type Description <code>SessionConfig</code> <p>A new <code>SessionConfig</code> object with the updated setting.</p>"},{"location":"api/context/#datafusion.SessionConfig.with_repartition_joins","title":"<code>datafusion.SessionConfig.with_repartition_joins(enabled)</code>","text":"<p>Enable or disable the use of repartitioning for joins to improve parallelism.</p> <p>Parameters:</p> Name Type Description Default <code>enabled</code> <code>bool</code> <p>Whether to use repartitioning for joins.</p> required <p>Returns:</p> Type Description <code>SessionConfig</code> <p>A new <code>SessionConfig</code> object with the updated setting.</p>"},{"location":"api/context/#datafusion.SessionConfig.with_repartition_sorts","title":"<code>datafusion.SessionConfig.with_repartition_sorts(enabled)</code>","text":"<p>Enable or disable the use of repartitioning for window functions to improve parallelism.</p> <p>Parameters:</p> Name Type Description Default <code>enabled</code> <code>bool</code> <p>Whether to use repartitioning for window functions.</p> required <p>Returns:</p> Type Description <code>SessionConfig</code> <p>A new <code>SessionConfig</code> object with the updated setting.</p>"},{"location":"api/context/#datafusion.SessionConfig.with_repartition_windows","title":"<code>datafusion.SessionConfig.with_repartition_windows(enabled)</code>","text":"<p>Enable or disable the use of repartitioning for window functions to improve parallelism.</p> <p>Parameters:</p> Name Type Description Default <code>enabled</code> <code>bool</code> <p>Whether to use repartitioning for window functions.</p> required <p>Returns:</p> Type Description <code>SessionConfig</code> <p>A new <code>SessionConfig</code> object with the updated setting.</p>"},{"location":"api/context/#datafusion.SessionConfig.with_target_partitions","title":"<code>datafusion.SessionConfig.with_target_partitions(target_partitions)</code>","text":"<p>Customize the number of target partitions for query execution.</p> <p>Increasing partitions can increase concurrency.</p> <p>Parameters:</p> Name Type Description Default <code>target_partitions</code> <code>int</code> <p>Number of target partitions.</p> required <p>Returns:</p> Type Description <code>SessionConfig</code> <p>A new <code>SessionConfig</code> object with the updated setting.</p>"},{"location":"api/context/#datafusion.RuntimeConfig","title":"<code>datafusion.RuntimeConfig</code>","text":""},{"location":"api/context/#datafusion.RuntimeConfig-functions","title":"Functions","text":""},{"location":"api/context/#datafusion.RuntimeConfig.__init__","title":"<code>datafusion.RuntimeConfig.__init__()</code>","text":"<p>Create a new <code>RuntimeConfig</code> with default values.</p>"},{"location":"api/context/#datafusion.RuntimeConfig.with_disk_manager_disabled","title":"<code>datafusion.RuntimeConfig.with_disk_manager_disabled()</code>","text":"<p>Disable the disk manager, attempts to create temporary files will error.</p> <p>Returns:</p> Type Description <code>RuntimeConfig</code> <p>A new <code>RuntimeConfig</code> object with the updated setting.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; config = RuntimeConfig().with_disk_manager_disabled()\n</code></pre>"},{"location":"api/context/#datafusion.RuntimeConfig.with_disk_manager_os","title":"<code>datafusion.RuntimeConfig.with_disk_manager_os()</code>","text":"<p>Use the operating system's temporary directory for disk manager.</p> <p>Returns:</p> Type Description <code>RuntimeConfig</code> <p>A new <code>RuntimeConfig</code> object with the updated setting.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; config = RuntimeConfig().with_disk_manager_os()\n</code></pre>"},{"location":"api/context/#datafusion.RuntimeConfig.with_disk_manager_specified","title":"<code>datafusion.RuntimeConfig.with_disk_manager_specified(paths)</code>","text":"<p>Use the specified paths for the disk manager's temporary files.</p> <p>Parameters:</p> Name Type Description Default <code>paths</code> <code>list[str]</code> <p>Paths to use for the disk manager's temporary files.</p> required <p>Returns:</p> Type Description <code>RuntimeConfig</code> <p>A new <code>RuntimeConfig</code> object with the updated setting.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; config = RuntimeConfig().with_disk_manager_specified([\"/tmp\"])\n</code></pre>"},{"location":"api/context/#datafusion.RuntimeConfig.with_fair_spill_pool","title":"<code>datafusion.RuntimeConfig.with_fair_spill_pool(size)</code>","text":"<p>Use a fair spill pool with the specified size.</p> <p>This pool works best when you know beforehand the query has multiple spillable operators that will likely all need to spill. Sometimes it will cause spills even when there was sufficient memory (reserved for other operators) to avoid doing so.</p> <pre><code>    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500z\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500z\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502                       z                      z               \u2502\n    \u2502                       z                      z               \u2502\n    \u2502       Spillable       z       Unspillable    z     Free      \u2502\n    \u2502        Memory         z        Memory        z    Memory     \u2502\n    \u2502                       z                      z               \u2502\n    \u2502                       z                      z               \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500z\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500z\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>size</code> <code>int</code> <p>Size of the memory pool in bytes.</p> required <p>Returns:</p> Type Description <code>RuntimeConfig</code> <p>A new <code>RuntimeConfig</code> object with the updated setting.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; config = RuntimeConfig().with_fair_spill_pool(1024)\n</code></pre>"},{"location":"api/context/#datafusion.RuntimeConfig.with_greedy_memory_pool","title":"<code>datafusion.RuntimeConfig.with_greedy_memory_pool(size)</code>","text":"<p>Use a greedy memory pool with the specified size.</p> <p>This pool works well for queries that do not need to spill or have a single spillable operator. See <code>RuntimeConfig.with_fair_spill_pool</code> if there are multiple spillable operators that all will spill.</p> <p>Parameters:</p> Name Type Description Default <code>size</code> <code>int</code> <p>Size of the memory pool in bytes.</p> required <p>Returns:</p> Type Description <code>RuntimeConfig</code> <p>A new <code>RuntimeConfig</code> object with the updated setting.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; config = RuntimeConfig().with_greedy_memory_pool(1024)\n</code></pre>"},{"location":"api/context/#datafusion.RuntimeConfig.with_temp_file_path","title":"<code>datafusion.RuntimeConfig.with_temp_file_path(path)</code>","text":"<p>Use the specified path to create any needed temporary files.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to use for temporary files.</p> required <p>Returns:</p> Type Description <code>RuntimeConfig</code> <p>A new <code>RuntimeConfig</code> object with the updated setting.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; config = RuntimeConfig().with_temp_file_path(\"/tmp\")\n</code></pre>"},{"location":"api/context/#datafusion.RuntimeConfig.with_unbounded_memory_pool","title":"<code>datafusion.RuntimeConfig.with_unbounded_memory_pool()</code>","text":"<p>Use an unbounded memory pool.</p> <p>Returns:</p> Type Description <code>RuntimeConfig</code> <p>A new <code>RuntimeConfig</code> object with the updated setting.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; config = RuntimeConfig().with_unbounded_memory_pool()\n</code></pre>"},{"location":"api/context/#datafusion.SQLOptions","title":"<code>datafusion.SQLOptions</code>","text":""},{"location":"api/context/#datafusion.SQLOptions-functions","title":"Functions","text":""},{"location":"api/context/#datafusion.SQLOptions.__init__","title":"<code>datafusion.SQLOptions.__init__()</code>","text":"<p>Create a new <code>SQLOptions</code> with default values.</p> <p>The default values are: - DDL commands are allowed - DML commands are allowed - Statements are allowed</p>"},{"location":"api/context/#datafusion.SQLOptions.with_allow_ddl","title":"<code>datafusion.SQLOptions.with_allow_ddl(allow)</code>","text":"<p>Should DDL (Data Definition Language) commands be run?</p> <p>Examples of DDL commands include <code>CREATE TABLE</code> and <code>DROP TABLE</code>.</p> <p>Parameters:</p> Name Type Description Default <code>allow</code> <code>bool</code> <p>Allow DDL commands to be run.</p> required <p>Returns:</p> Type Description <code>SQLOptions</code> <p>A new <code>SQLOptions</code> object with the updated setting.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; options = SQLOptions().with_allow_ddl(True)\n</code></pre>"},{"location":"api/context/#datafusion.SQLOptions.with_allow_dml","title":"<code>datafusion.SQLOptions.with_allow_dml(allow)</code>","text":"<p>Should DML (Data Manipulation Language) commands be run?</p> <p>Examples of DML commands include <code>INSERT INTO</code> and <code>DELETE</code>.</p> <p>Parameters:</p> Name Type Description Default <code>allow</code> <code>bool</code> <p>Allow DML commands to be run.</p> required <p>Returns:</p> Type Description <code>SQLOptions</code> <p>A new <code>SQLOptions</code> object with the updated setting.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; options = SQLOptions().with_allow_dml(True)\n</code></pre>"},{"location":"api/context/#datafusion.SQLOptions.with_allow_statements","title":"<code>datafusion.SQLOptions.with_allow_statements(allow)</code>","text":"<p>Should statements such as <code>SET VARIABLE</code> and <code>BEGIN TRANSACTION</code> be run?</p> <p>Parameters:</p> Name Type Description Default <code>allow</code> <code>bool</code> <p>Allow statements to be run.</p> required <p>Returns:</p> Type Description <code>SQLOptions</code> <p>A new <code>SQLOptions</code> object with the updated setting.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; options = SQLOptions().with_allow_statements(True)\n</code></pre>"},{"location":"api/dataframe/","title":"DataFrame","text":""},{"location":"api/dataframe/#datafusion.DataFrame","title":"<code>datafusion.DataFrame</code>","text":""},{"location":"api/dataframe/#datafusion.DataFrame-functions","title":"Functions","text":""},{"location":"api/dataframe/#datafusion.DataFrame.__getitem__","title":"<code>datafusion.DataFrame.__getitem__(key)</code>","text":"<p>Return a new <code>DataFrame</code> with the specified column or columns.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>Any</code> <p>Column name or list of column names to select.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with the specified column or columns.</p>"},{"location":"api/dataframe/#datafusion.DataFrame.__repr__","title":"<code>datafusion.DataFrame.__repr__()</code>","text":"<p>Return a string representation of the DataFrame.</p> <p>Returns:</p> Type Description <code>str</code> <p>String representation of the DataFrame.</p>"},{"location":"api/dataframe/#datafusion.DataFrame.aggregate","title":"<code>datafusion.DataFrame.aggregate(group_by, aggs)</code>","text":"<p>Return a new <code>DataFrame</code> that aggregates the rows of the current DataFrame.</p> <p>First optionally grouping by the given expressions.</p> <p>Parameters:</p> Name Type Description Default <code>group_by</code> <code>list[Expr]</code> <p>List of expressions to group by.</p> required <code>aggs</code> <code>list[Expr]</code> <p>List of expressions to aggregate.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame after aggregation.</p>"},{"location":"api/dataframe/#datafusion.DataFrame.cache","title":"<code>datafusion.DataFrame.cache()</code>","text":"<p>Cache the DataFrame as a memory table.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Cached DataFrame.</p>"},{"location":"api/dataframe/#datafusion.DataFrame.collect","title":"<code>datafusion.DataFrame.collect()</code>","text":"<p>Execute this <code>DataFrame</code> and collect <code>pyarrow.RecordBatch</code>es into memory.</p> <p>Prior to calling <code>collect</code>, modifying a DataFrme simply updates a plan (no actual computation is performed). Calling <code>collect</code> triggers the computation.</p> <p>Returns:</p> Type Description <code>list[RecordBatch]</code> <p>List of <code>pyarrow.RecordBatch</code>es collected from the DataFrame.</p>"},{"location":"api/dataframe/#datafusion.DataFrame.collect_partitioned","title":"<code>datafusion.DataFrame.collect_partitioned()</code>","text":"<p>Execute this DataFrame and collect all results into a list of list of <code>pyarrow.RecordBatch</code>es maintaining the input partitioning.</p> <p>Returns:</p> Type Description <code>list[list[RecordBatch]]</code> <p>List of list of <code>pyarrow.RecordBatch</code>es collected from the DataFrame.</p>"},{"location":"api/dataframe/#datafusion.DataFrame.count","title":"<code>datafusion.DataFrame.count()</code>","text":"<p>Return the total number of rows in this <code>DataFrame</code>.</p> <p>Note that this method will actually run a plan to calculate the count, which may be slow for large or complicated DataFrames.</p> <p>Returns:</p> Type Description <code>int</code> <p>Number of rows in the DataFrame.</p>"},{"location":"api/dataframe/#datafusion.DataFrame.describe","title":"<code>datafusion.DataFrame.describe()</code>","text":"<p>Return a new <code>DataFrame</code> that has statistics for a DataFrame.</p> <p>Only summarized numeric datatypes at the moments and returns nulls for non-numeric datatypes.</p> <p>The output format is modeled after pandas.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A summary DataFrame containing statistics.</p>"},{"location":"api/dataframe/#datafusion.DataFrame.distinct","title":"<code>datafusion.DataFrame.distinct()</code>","text":"<p>Return a new <code>DataFrame</code> with all duplicated rows removed.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame after removing duplicates.</p>"},{"location":"api/dataframe/#datafusion.DataFrame.except_all","title":"<code>datafusion.DataFrame.except_all(py_df)</code>","text":"<p>Calculate the exception of two <code>DataFrame</code>s.</p> <p>The two <code>DataFrame</code>s must have exactly the same schema.</p> <p>Parameters:</p> Name Type Description Default <code>py_df</code> <code>DataFrame</code> <p>DataFrame to calculate exception with.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame after exception.</p>"},{"location":"api/dataframe/#datafusion.DataFrame.execution_plan","title":"<code>datafusion.DataFrame.execution_plan()</code>","text":"<p>Return the execution/physical plan that comprises this <code>DataFrame</code>.</p> <p>Returns:</p> Type Description <code>ExecutionPlan</code> <p>Execution plan.</p>"},{"location":"api/dataframe/#datafusion.DataFrame.explain","title":"<code>datafusion.DataFrame.explain(verbose=False, analyze=False)</code>","text":"<p>Return a DataFrame with the explanation of its plan so far.</p> <p>If <code>analyze</code> is specified, runs the plan and reports metrics.</p> <p>Parameters:</p> Name Type Description Default <code>verbose</code> <code>bool</code> <p>If <code>True</code>, more details will be included, by default False</p> <code>False</code> <code>analyze</code> <code>bool</code> <p>If <code>True</code>, the plan will run and metrics reported, by default False</p> <code>False</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with the explanation of its plan.</p>"},{"location":"api/dataframe/#datafusion.DataFrame.filter","title":"<code>datafusion.DataFrame.filter(predicate)</code>","text":"<p>Return a DataFrame for which <code>predicate</code> evaluates to <code>True</code>.</p> <p>Rows for which <code>predicate</code> evaluates to <code>False</code> or <code>None</code> are filtered out.</p> <p>Parameters:</p> Name Type Description Default <code>predicate</code> <code>Expr</code> <p>Predicate expression to filter the DataFrame.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame after filtering.</p>"},{"location":"api/dataframe/#datafusion.DataFrame.intersect","title":"<code>datafusion.DataFrame.intersect(py_df)</code>","text":"<p>Calculate the intersection of two <code>DataFrame</code>s.</p> <p>The two <code>DataFrame</code>s must have exactly the same schema.</p> <p>Parameters:</p> Name Type Description Default <code>py_df</code> <code>DataFrame</code> <p>DataFrame to intersect with.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame after intersection.</p>"},{"location":"api/dataframe/#datafusion.DataFrame.join","title":"<code>datafusion.DataFrame.join(right, join_keys, how)</code>","text":"<p>Join this <code>DataFrame</code> with another <code>DataFrame</code>  using explicitly specified columns.</p> <p>Parameters:</p> Name Type Description Default <code>right</code> <code>DataFrame</code> <p>Other DataFrame to join with.</p> required <code>join_keys</code> <code>tuple[list[str], list[str]]</code> <p>Tuple of two lists of column names to join on.</p> required <code>how</code> <code>str</code> <p>Type of join to perform. Supported types are \"inner\", \"left\", \"right\", \"full\", \"semi\", \"anti\".</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame after join.</p>"},{"location":"api/dataframe/#datafusion.DataFrame.limit","title":"<code>datafusion.DataFrame.limit(count, offset=0)</code>","text":"<p>Return a new <code>DataFrame</code> with a limited number of rows.</p> <p>Parameters:</p> Name Type Description Default <code>count</code> <code>int</code> <p>Number of rows to limit the DataFrame to.</p> required <code>offset</code> <code>int</code> <p>Number of rows to skip, by default 0</p> <code>0</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame after limiting.</p>"},{"location":"api/dataframe/#datafusion.DataFrame.logical_plan","title":"<code>datafusion.DataFrame.logical_plan()</code>","text":"<p>Return the unoptimized <code>LogicalPlan</code> that comprises this <code>DataFrame</code>.</p> <p>Returns:</p> Type Description <code>LogicalPlan</code> <p>Unoptimized logical plan.</p>"},{"location":"api/dataframe/#datafusion.DataFrame.optimized_logical_plan","title":"<code>datafusion.DataFrame.optimized_logical_plan()</code>","text":"<p>Return the optimized <code>LogicalPlan</code> that comprises this <code>DataFrame</code>.</p> <p>Returns:</p> Type Description <code>LogicalPlan</code> <p>Optimized logical plan.</p>"},{"location":"api/dataframe/#datafusion.DataFrame.repartition","title":"<code>datafusion.DataFrame.repartition(num)</code>","text":"<p>Repartition a DataFrame into <code>num</code> partitions.</p> <p>The batches allocation uses a round-robin algorithm.</p> <p>Parameters:</p> Name Type Description Default <code>num</code> <code>int</code> <p>Number of partitions to repartition the DataFrame into.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>Repartitioned DataFrame.</p>"},{"location":"api/dataframe/#datafusion.DataFrame.repartition_by_hash","title":"<code>datafusion.DataFrame.repartition_by_hash(*args, num)</code>","text":"<p>Repartition a DataFrame into <code>num</code> partitions using a hash partitioning scheme.</p> <p>Parameters:</p> Name Type Description Default <code>num</code> <code>int</code> <p>Number of partitions to repartition the DataFrame into.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>Repartitioned DataFrame.</p>"},{"location":"api/dataframe/#datafusion.DataFrame.schema","title":"<code>datafusion.DataFrame.schema()</code>","text":"<p>Return the <code>pyarrow.Schema</code> describing the output of this DataFrame.</p> <p>The output schema contains information on the name, data type, and nullability for each column.</p> <p>Returns:</p> Type Description <code>Schema</code> <p>Describing schema of the DataFrame</p>"},{"location":"api/dataframe/#datafusion.DataFrame.select","title":"<code>datafusion.DataFrame.select(*args)</code>","text":"<p>Project arbitrary expressions (like SQL SELECT expressions) into a new <code>DataFrame</code>.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame after projection. It has one column for each expression.</p>"},{"location":"api/dataframe/#datafusion.DataFrame.select_columns","title":"<code>datafusion.DataFrame.select_columns(*args)</code>","text":"<p>Filter the DataFrame by columns.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame only containing the specified columns.</p>"},{"location":"api/dataframe/#datafusion.DataFrame.show","title":"<code>datafusion.DataFrame.show(num=20)</code>","text":"<p>Execute the DataFrame and print the result to the console.</p> <p>Parameters:</p> Name Type Description Default <code>num</code> <code>int</code> <p>Number of lines to show, by default 20</p> <code>20</code>"},{"location":"api/dataframe/#datafusion.DataFrame.sort","title":"<code>datafusion.DataFrame.sort(*exprs)</code>","text":"<p>Sort the DataFrame by the specified sorting expressions.</p> <p>Note that any expression can be turned into a sort expression by calling its <code>sort</code>  method.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame after sorting.</p>"},{"location":"api/dataframe/#datafusion.DataFrame.to_arrow_table","title":"<code>datafusion.DataFrame.to_arrow_table()</code>","text":"<p>Execute the <code>DataFrame</code> and convert it into an Arrow Table.</p> <p>Returns:</p> Type Description <code>Table</code> <p>Arrow Table.</p>"},{"location":"api/dataframe/#datafusion.DataFrame.to_pandas","title":"<code>datafusion.DataFrame.to_pandas()</code>","text":"<p>Execute the <code>DataFrame</code> and convert it into a Pandas DataFrame.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Pandas DataFrame.</p>"},{"location":"api/dataframe/#datafusion.DataFrame.to_polars","title":"<code>datafusion.DataFrame.to_polars()</code>","text":"<p>Execute the <code>DataFrame</code> and convert it into a Polars DataFrame.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Polars DataFrame.</p>"},{"location":"api/dataframe/#datafusion.DataFrame.to_pydict","title":"<code>datafusion.DataFrame.to_pydict()</code>","text":"<p>Execute the <code>DataFrame</code> and convert it into a dictionary of lists.</p> <p>Returns:</p> Type Description <code>dict[str, list[Any]]</code> <p>Dictionary of lists.</p>"},{"location":"api/dataframe/#datafusion.DataFrame.to_pylist","title":"<code>datafusion.DataFrame.to_pylist()</code>","text":"<p>Execute the <code>DataFrame</code> and convert it into a list of dictionaries.</p> <p>Returns:</p> Type Description <code>list[dict[str, Any]]</code> <p>List of dictionaries.</p>"},{"location":"api/dataframe/#datafusion.DataFrame.union","title":"<code>datafusion.DataFrame.union(py_df, distinct=False)</code>","text":"<p>Calculate the union of two <code>DataFrame</code>s.</p> <p>The two <code>DataFrame</code>s must have exactly the same schema.</p> <p>Parameters:</p> Name Type Description Default <code>py_df</code> <code>DataFrame</code> <p>DataFrame to union with.</p> required <code>distinct</code> <code>bool</code> <p>If <code>True</code>, duplicate rows will be removed, by default False</p> <code>False</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame after union.</p>"},{"location":"api/dataframe/#datafusion.DataFrame.union_distinct","title":"<code>datafusion.DataFrame.union_distinct(py_df)</code>","text":"<p>Calculate the distinct union of two <code>DataFrame</code>s.</p> <p>The two <code>DataFrame</code>s must have exactly the same schema. Any duplicate rows are discarded.</p> <p>Parameters:</p> Name Type Description Default <code>py_df</code> <code>DataFrame</code> <p>DataFrame to union with.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame after union.</p>"},{"location":"api/dataframe/#datafusion.DataFrame.with_column","title":"<code>datafusion.DataFrame.with_column(name, expr)</code>","text":"<p>Add an additional column to the DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the column to add.</p> required <code>expr</code> <code>Expr</code> <p>Expression to compute the column.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with the new column.</p>"},{"location":"api/dataframe/#datafusion.DataFrame.with_column_renamed","title":"<code>datafusion.DataFrame.with_column_renamed(old_name, new_name)</code>","text":"<p>Rename one column by applying a new projection.</p> <p>This is a no-op if the column to be renamed does not exist.</p> <p>The method supports case sensitive rename with wrapping column name into one the following symbols (\" or ' or `).</p> <p>Parameters:</p> Name Type Description Default <code>old_name</code> <code>str</code> <p>Old column name.</p> required <code>new_name</code> <code>str</code> <p>New column name.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with the column renamed.</p>"},{"location":"api/dataframe/#datafusion.DataFrame.write_csv","title":"<code>datafusion.DataFrame.write_csv(path)</code>","text":"<p>Execute the <code>DataFrame</code>  and write the results to a CSV file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path of the CSV file to write.</p> required"},{"location":"api/dataframe/#datafusion.DataFrame.write_json","title":"<code>datafusion.DataFrame.write_json(path)</code>","text":"<p>Execute the <code>DataFrame</code> and write the results to a JSON file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path of the JSON file to write.</p> required"},{"location":"api/dataframe/#datafusion.DataFrame.write_parquet","title":"<code>datafusion.DataFrame.write_parquet(path, compression='uncompressed', compression_level=None)</code>","text":"<p>Execute the <code>DataFrame</code> and write the results to a Parquet file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path of the Parquet file to write.</p> required <code>compression</code> <code>str</code> <p>Compression type to use, by default \"uncompressed\"</p> <code>'uncompressed'</code> <code>compression_level</code> <code>int | None</code> <p>Compression level to use, by default None</p> <code>None</code>"},{"location":"api/substrait/","title":"Substrait","text":""},{"location":"api/substrait/#datafusion.substrait.plan","title":"<code>datafusion.substrait.plan</code>","text":""},{"location":"api/substrait/#datafusion.substrait.plan-functions","title":"Functions","text":""},{"location":"api/substrait/#datafusion.substrait.plan.encode","title":"<code>datafusion.substrait.plan.encode()</code>","text":"<p>Encode the plan to bytes.</p> <p>Returns:</p> Type Description <code>bytes</code> <p>Encoded plan.</p>"},{"location":"api/substrait/#datafusion.substrait.serde","title":"<code>datafusion.substrait.serde</code>","text":""},{"location":"api/substrait/#datafusion.substrait.serde-functions","title":"Functions","text":""},{"location":"api/substrait/#datafusion.substrait.serde.deserialize","title":"<code>datafusion.substrait.serde.deserialize(path)</code>  <code>staticmethod</code>","text":"<p>Deserialize a Substrait plan from a file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to read the Substrait plan from.</p> required <p>Returns:</p> Type Description <code>plan</code> <p>Substrait plan.</p>"},{"location":"api/substrait/#datafusion.substrait.serde.deserialize_bytes","title":"<code>datafusion.substrait.serde.deserialize_bytes(proto_bytes)</code>  <code>staticmethod</code>","text":"<p>Deserialize a Substrait plan from bytes.</p> <p>Parameters:</p> Name Type Description Default <code>proto_bytes</code> <code>bytes</code> <p>Bytes to read the Substrait plan from.</p> required <p>Returns:</p> Type Description <code>plan</code> <p>Substrait plan.</p>"},{"location":"api/substrait/#datafusion.substrait.serde.serialize","title":"<code>datafusion.substrait.serde.serialize(sql, ctx, path)</code>  <code>staticmethod</code>","text":"<p>Serialize a SQL query to a Substrait plan and write it to a file.</p> <p>Parameters:</p> Name Type Description Default <code>sql</code> <code>str</code> <p>SQL query to serialize.</p> required <code>ctx</code> <code>SessionContext</code> <p>SessionContext to use.</p> required <code>path</code> <code>str</code> <p>Path to write the Substrait plan to.</p> required"},{"location":"api/substrait/#datafusion.substrait.serde.serialize_bytes","title":"<code>datafusion.substrait.serde.serialize_bytes(sql, ctx)</code>  <code>staticmethod</code>","text":"<p>Serialize a SQL query to a Substrait plan as bytes.</p> <p>Parameters:</p> Name Type Description Default <code>sql</code> <code>str</code> <p>SQL query to serialize.</p> required <code>ctx</code> <code>SessionContext</code> <p>SessionContext to use.</p> required <p>Returns:</p> Type Description <code>bytes</code> <p>Substrait plan as bytes.</p>"},{"location":"api/substrait/#datafusion.substrait.serde.serialize_to_plan","title":"<code>datafusion.substrait.serde.serialize_to_plan(sql, ctx)</code>  <code>staticmethod</code>","text":"<p>Serialize a SQL query to a Substrait plan.</p> <p>Parameters:</p> Name Type Description Default <code>sql</code> <code>str</code> <p>SQL query to serialize.</p> required <code>ctx</code> <code>SessionContext</code> <p>SessionContext to use.</p> required <p>Returns:</p> Type Description <code>plan</code> <p>Substrait plan.</p>"},{"location":"api/substrait/#datafusion.substrait.producer","title":"<code>datafusion.substrait.producer</code>","text":""},{"location":"api/substrait/#datafusion.substrait.producer-functions","title":"Functions","text":""},{"location":"api/substrait/#datafusion.substrait.producer.to_substrait_plan","title":"<code>datafusion.substrait.producer.to_substrait_plan(plan, ctx)</code>  <code>staticmethod</code>","text":"<p>Convert a DataFusion LogicalPlan to a Substrait plan.</p> <p>Parameters:</p> Name Type Description Default <code>plan</code> <code>LogicalPlan</code> <p>LogicalPlan to convert.</p> required <code>ctx</code> <code>SessionContext</code> <p>SessionContext to use.</p> required <p>Returns:</p> Type Description <code>plan</code> <p>Substrait plan.</p>"},{"location":"api/substrait/#datafusion.substrait.consumer","title":"<code>datafusion.substrait.consumer</code>","text":""},{"location":"api/substrait/#datafusion.substrait.consumer-functions","title":"Functions","text":""},{"location":"api/substrait/#datafusion.substrait.consumer.from_substrait_plan","title":"<code>datafusion.substrait.consumer.from_substrait_plan(ctx, plan)</code>  <code>staticmethod</code>","text":"<p>Convert a Substrait plan to a DataFusion LogicalPlan.</p> <p>Parameters:</p> Name Type Description Default <code>ctx</code> <code>SessionContext</code> <p>SessionContext to use.</p> required <code>plan</code> <code>plan</code> <p>Substrait plan to convert.</p> required <p>Returns:</p> Type Description <code>LogicalPlan</code> <p>LogicalPlan.</p>"}]}